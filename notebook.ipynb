{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bb59b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-07 22:02:13.483389: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2026-01-07 22:02:14.019463: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-07 22:02:16.319585: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "/home/austine/Desktop/music-vibes-classification/env/lib/python3.13/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n",
      "/home/austine/Desktop/music-vibes-classification/env/lib/python3.13/site-packages/tensorflow_hub/__init__.py:61: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import parse_version\n",
      "2026-01-07 22:02:18.809078: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers, models\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "yamnet_model = hub.load(\"https://tfhub.dev/google/yamnet/1\")\n",
    "\n",
    "# def extract_song_embeddings(file_path):\n",
    "#     try:\n",
    "#         waveform, _ = librosa.load(file_path, sr=16000, mono=True)\n",
    "#         waveform = waveform.astype(np.float32)\n",
    "\n",
    "#         _, embbedings, _ = yamnet_model(waveform)\n",
    "#         song_embeddings = tf.reduce_mean(embbedings, axis=0)\n",
    "        \n",
    "#         return song_embeddings.numpy()\n",
    "#     except:\n",
    "#         print(f\"Error has occured in file path {file_path}\")\n",
    "#         return None\n",
    "    \n",
    "def extract_song_embeddings(segment):\n",
    "    _, embbedings, _ = yamnet_model(segment)\n",
    "    song_embeddings = tf.reduce_mean(embbedings, axis=0)\n",
    "        \n",
    "    return song_embeddings.numpy()\n",
    "\n",
    "def extract_song_segments(file_path):\n",
    "    segment_seconds = 5\n",
    "    overlap_seconds = 0 \n",
    "    segments = []\n",
    "\n",
    "    try:\n",
    "        waveform, sr = librosa.load(file_path, sr=16000, mono=True)\n",
    "\n",
    "        segment_samples = segment_seconds * sr\n",
    "        hop_samples = segment_samples - int(overlap_seconds * sr)\n",
    "\n",
    "        for start in range(0, \n",
    "                           len(waveform) - segment_samples + 1, \n",
    "                           hop_samples):\n",
    "                           \n",
    "            segment = waveform[start: start + segment_samples]\n",
    "            segments.append(segment)\n",
    "\n",
    "        return segments\n",
    "    \n",
    "    except:\n",
    "        print(f\"Error has occured in file path {file_path}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e70cbc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting songs embedding in folder chill: 100%|██████████| 200/200 [01:20<00:00,  2.48it/s]\n",
      "Extracting songs embedding in folder chaotic: 100%|██████████| 200/200 [00:54<00:00,  3.70it/s]\n",
      "Extracting songs embedding in folder energetic: 100%|██████████| 300/300 [01:12<00:00,  4.12it/s]\n",
      "Extracting songs embedding in folder relaxing:  39%|███▉      | 117/300 [00:32<00:40,  4.48it/s]/tmp/ipykernel_21677/1666016675.py:39: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  waveform, sr = librosa.load(file_path, sr=16000, mono=True)\n",
      "/home/austine/Desktop/music-vibes-classification/env/lib/python3.13/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "Extracting songs embedding in folder relaxing:  39%|███▉      | 118/300 [00:32<00:40,  4.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error has occured in file path vibes/relaxing/jazz.00054.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting songs embedding in folder relaxing: 100%|██████████| 300/300 [01:18<00:00,  3.84it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"vibes\"\n",
    "dataset_csv_path = \"song_embeddings.csv\"\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "vibe_map = {\"relaxing\": 0,\n",
    "             \"chaotic\": 1,\n",
    "             \"chill\": 2,\n",
    "             \"energetic\": 3}\n",
    "\n",
    "if not os.path.exists(dataset_csv_path):\n",
    "    for vibe in os.listdir(dataset_path):\n",
    "        vibe_path = os.path.join(dataset_path, vibe)\n",
    "        for file in tqdm(os.listdir(vibe_path), desc=f\"Extracting songs embedding in folder {vibe}\"):\n",
    "            file_path = os.path.join(vibe_path, file)\n",
    "            segments = extract_song_segments(file_path)\n",
    "            if segments is not None:\n",
    "                for segment in segments:\n",
    "                    embeddings = extract_song_embeddings(segment)\n",
    "                    X.append(embeddings)\n",
    "                    y.append(vibe_map[vibe])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6624f746",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(dataset_csv_path):\n",
    "    df = pd.DataFrame(X)\n",
    "    df[\"label\"] = y\n",
    "\n",
    "    df.to_csv(dataset_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75dc59fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.8151 - loss: 0.5108 - val_accuracy: 0.8789 - val_loss: 0.6103\n",
      "Epoch 2/50\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8658 - loss: 0.3515 - val_accuracy: 0.8820 - val_loss: 0.4148\n",
      "Epoch 3/50\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8815 - loss: 0.3124 - val_accuracy: 0.8967 - val_loss: 0.3309\n",
      "Epoch 4/50\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8966 - loss: 0.2786 - val_accuracy: 0.8914 - val_loss: 0.3073\n",
      "Epoch 5/50\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8979 - loss: 0.2690 - val_accuracy: 0.8789 - val_loss: 0.3367\n",
      "Epoch 6/50\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9060 - loss: 0.2457 - val_accuracy: 0.9061 - val_loss: 0.2719\n",
      "Epoch 7/50\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9159 - loss: 0.2206 - val_accuracy: 0.8841 - val_loss: 0.3280\n",
      "Epoch 8/50\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9238 - loss: 0.2025 - val_accuracy: 0.9102 - val_loss: 0.2765\n",
      "Epoch 9/50\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9256 - loss: 0.2027 - val_accuracy: 0.8810 - val_loss: 0.3059\n",
      "Epoch 10/50\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9300 - loss: 0.1878 - val_accuracy: 0.9071 - val_loss: 0.2682\n",
      "Epoch 11/50\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9389 - loss: 0.1637 - val_accuracy: 0.9165 - val_loss: 0.2584\n",
      "Epoch 12/50\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9423 - loss: 0.1642 - val_accuracy: 0.9019 - val_loss: 0.2743\n",
      "Epoch 13/50\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9452 - loss: 0.1537 - val_accuracy: 0.9144 - val_loss: 0.2473\n",
      "Epoch 14/50\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9441 - loss: 0.1506 - val_accuracy: 0.8946 - val_loss: 0.3039\n",
      "Epoch 15/50\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9493 - loss: 0.1407 - val_accuracy: 0.9144 - val_loss: 0.2536\n",
      "Epoch 16/50\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9446 - loss: 0.1488 - val_accuracy: 0.9134 - val_loss: 0.2947\n",
      "Epoch 17/50\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9569 - loss: 0.1181 - val_accuracy: 0.9196 - val_loss: 0.2525\n",
      "Epoch 18/50\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9533 - loss: 0.1210 - val_accuracy: 0.9050 - val_loss: 0.2496\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(dataset_csv_path,\n",
    "                 index_col=0)\n",
    "\n",
    "X = np.array(df.drop(columns=[\"label\"]))\n",
    "y = np.array(df[\"label\"])\n",
    "\n",
    "vibe_labels = [\"Relaxing\", \n",
    "               \"Chaotic\", \n",
    "               \"Chill\", \n",
    "               \"Energetic\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    random_state=42, \n",
    "                                                    test_size=0.20,\n",
    "                                                    stratify=y)\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(1024,)),\n",
    "    layers.Dense(256, activation=\"relu\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(128, activation=\"relu\"),\n",
    "    layers.Dense(len(vibe_labels), activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            patience=5,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf706471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Relaxing       0.95      0.97      0.96       358\n",
      "     Chaotic       0.85      0.90      0.87       240\n",
      "       Chill       0.89      0.90      0.90       240\n",
      "   Energetic       0.91      0.86      0.89       359\n",
      "\n",
      "    accuracy                           0.91      1197\n",
      "   macro avg       0.90      0.91      0.90      1197\n",
      "weighted avg       0.91      0.91      0.91      1197\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_preds = model.predict(X_test)\n",
    "y_pred_labels = np.argmax(y_preds, axis=1)\n",
    "\n",
    "print(classification_report(y_test,\n",
    "                            y_pred_labels,\n",
    "                            target_names=vibe_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3de439eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Predicted Class: Energetic\n"
     ]
    }
   ],
   "source": [
    "z = []\n",
    "song_segments = extract_song_segments(\"testing_dataset/kehlani.mp3\")\n",
    "\n",
    "for song_segment in song_segments:\n",
    "    song_embeddings = extract_song_embeddings(song_segment)\n",
    "    z.append(song_embeddings)\n",
    "\n",
    "z = np.array(z)\n",
    "\n",
    "y_pred = model.predict(z)\n",
    "\n",
    "song_pred = np.mean(y_pred, axis=0)\n",
    "y_pred_label = np.argmax(song_pred)\n",
    "\n",
    "print(\"Predicted Class:\", vibe_labels[y_pred_label])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
